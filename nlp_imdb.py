# -*- coding: utf-8 -*-
"""NLP_IMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v7_5fnCIvM6WkVUTmIZgF9qp4dXDlx5C
"""

from google.colab import drive
drive.mount('/content/gdrive')

import io
import pandas as pd
import numpy as np
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

data = pd.read_csv(r'gdrive/My Drive/Data Science Projects/IMDB/IMDB_Dataset.csv')

data

data.info()

data['sentiment'].value_counts()

review= data['review'][1]

review

"""Normally any NLP task involves following text cleaning techniques -

1.   Removal of HTML contents like "< br>".
2.   Removal of punctutions, special characters like '\'.
3.   Removal of stopwords like is, the which do not offer much insight.
4.   Lemmatization to bring back multiple forms of same word to their common root like 'coming', 'comes' into 'come'.
5.   Vectorization - Encode the numeric values once you have cleaned it.
6.   Fit the data to the ML model.

Step 1: Removal of HTML content
"""

from bs4 import BeautifulSoup

review= BeautifulSoup(review,"html.parser").get_text()
review

"""Step 2: Removal of punctuations and special charcters."""

import re

review = re.sub('[^a-zA-Z]', ' ', review)
review

"""Bring everything into lowercase."""

review = review.lower()
review

"""Stopwords removal - since stopwords removal works on every word in your text we need to split the text."""

review = review.split()
review

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

review = [word for word in review if not word in set(stopwords.words('english'))]
review

"""Step 4: Lemmatization"""

from nltk.stem import WordNetLemmatizer

lem = WordNetLemmatizer()
review = [lem.lemmatize(word) for word in review]
review

"""Merge the words to form cleaned up version of the text."""

review = ' '.join(review)
review

"""Create a Corpus first because next step will be to bring this text in mathematical forms."""

corpus = []
corpus.append(review)

corpus

"""Step 5: Vectorization"""

from sklearn.feature_extraction.text import CountVectorizer

count_vec = CountVectorizer()
review_count_vec = count_vec.fit_transform(corpus)

review_count_vec.toarray()

"""The data has become numeric with 1,2 and 3s based on the number of times they appear in the text.
There is another variation of CountVectorizer with binary=True and in that case all non-zero entries will have 1.
"""

count_vec_bin = CountVectorizer(binary=True)
review_count_vec_bin = count_vec_bin.fit_transform(corpus)

review_count_vec_bin.toarray()

"""So there is no 2s and 3s in the vector.

Now I will appply TF-IDF - TF stands for Text Frequency which means how many times a word (term) appears in a text (document). IDF means Inverse Document Frequency and is calculated as log(# of documents in corpus/# of documents containing the term).

Finally TF-IDF score is calculated as TF * IDF.

IDF acts as a balancing factor and diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vec = TfidfVectorizer()
review_tfidf_vec = tfidf_vec.fit_transform(corpus)

review_tfidf_vec.toarray()

"""Split train-test dataset."""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.30)

"""Convert sentiments to numeric forms."""

y_train = (y_train.replace({'positive': 1, 'negative': 0})).values
y_test  = (y_test.replace({'positive': 1, 'negative': 0})).values

"""Apply the techniques of cleaning data on whole train and test dataset."""

corpus_train = []
corpus_test  = []

for i in range(X_train.shape[0]):
    soup = BeautifulSoup(X_train.iloc[i], "html.parser")
    review = soup.get_text()
    review = re.sub('[^a-zA-Z]', ' ', review)
    review = review.lower()
    review = review.split()
    review = [word for word in review if not word in set(stopwords.words('english'))]
    lem = WordNetLemmatizer()
    review = [lem.lemmatize(word) for word in review]
    review = ' '.join(review)
    corpus_train.append(review)

for j in range(X_test.shape[0]):
    soup = BeautifulSoup(X_test.iloc[j], "html.parser")
    review = soup.get_text()
    review = re.sub('[^a-zA-Z]', ' ', review)
    review = review.lower()
    review = review.split()
    review = [word for word in review if not word in set(stopwords.words('english'))]
    lem = WordNetLemmatizer()
    review = [lem.lemmatize(word) for word in review]
    review = ' '.join(review)
    corpus_test.append(review)

"""Vectorize using TF-IDF technique."""

tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))

tfidf_vec_train = tfidf_vec.fit_transform(corpus_train)
tfidf_vec_test = tfidf_vec.transform(corpus_test)

"""Step 6: Fit the data to ML model."""

from sklearn.svm import LinearSVC

linear_svc = LinearSVC(C=0.5, random_state=42)
linear_svc.fit(tfidf_vec_train, y_train)


y_predict = linear_svc.predict(tfidf_vec_test)

"""Let's see the performance."""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("Classification Report: \n", classification_report(y_test, y_predict,target_names=['Negative','Positive']))
print("Confusion Matrix: \n", confusion_matrix(y_test, y_predict))
print("Accuracy: \n", accuracy_score(y_test, y_predict))

"""Now we will vectorize using CountVectorizer(binary=False) and fit it on LinearSVC model."""

count_vec = CountVectorizer(ngram_range=(1, 3), binary=False)
count_vec_train = count_vec.fit_transform(corpus_train)
count_vec_test = count_vec.transform(corpus_test)

linear_svc_count = LinearSVC(C=0.5, random_state=42, max_iter=5000)
linear_svc_count.fit(count_vec_train, y_train)

predict_count = linear_svc_count.predict(count_vec_test)

print("Classification Report: \n", classification_report(y_test, predict_count,target_names=['Negative','Positive']))
print("Confusion Matrix: \n", confusion_matrix(y_test, predict_count))
print("Accuracy: \n", accuracy_score(y_test, predict_count))

"""So we are getting maximum accuracy using TF-IDF vectorizer."""

